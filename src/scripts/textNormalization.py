from collections import Counter
from gensim import corpora, models
from nltk.tokenize import RegexpTokenizer
import gensim, nltk


class textNormalization:
    """Manipula topic models"""

    def __init__(self):
        pass

    def escape_text_insert(self, text):
        return (
            text.replace('"', "")
            .replace("/", "")
            .replace("\\", "")
            .replace("<", "")
            .replace(">", "")
        )

    def file_to_string(self, arq):
        arquivo = open(arq, "r")
        return "".join([line for line in arquivo])

    def month_name_number(self, text):
        text = text.lower()
        if text == "janeiro":
            return "01"
        elif text == "fevereiro":
            return "02"
        elif text == "marÃ§o":
            return "03"
        elif text == "abril":
            return "04"
        elif text == "maio":
            return "05"
        elif text == "junho":
            return "06"
        elif text == "julho":
            return "07"
        elif text == "agosto":
            return "08"
        elif text == "setembro":
            return "09"
        elif text == "outubro":
            return "10"
        elif text == "novembro":
            return "09"
        elif text == "dezembro":
            return "12"

    def normalize_texts(self, texts, one_text=False):
        normal_texts = []
        tk = self.tokenizer()
        stopwords = nltk.corpus.stopwords.words("portuguese")
        if one_text:
            texts = [texts]
        for t in texts:
            texto_bruto = t.lower()
            tokens = tk.tokenize(texto_bruto)
            texto_processado = []
            for tkn in tokens:
                if len(tkn) > 3 and tkn not in stopwords:
                    try:
                        float(tkn)
                    except:
                        texto_processado.append(tkn)
            normal_texts.append(texto_processado)
        if one_text:
            return texto_processado
        return normal_texts

    def dicionario_invertido_id_texto(self, dados):
        dicionario_i = {}
        for id_, texto in dados:
            dicionario_i_text = set(self.normalize_texts(texto, one_text=True))
            for w in dicionario_i_text:
                if w in dicionario_i:
                    dicionario_i[w].append(id_)
                else:
                    dicionario_i[w] = [id_]
        return dicionario_i

    def text_to_hist(self, text):
        list_words = self.normalize_texts(text, one_text=True)
        return dict(Counter(list_words))

    def tokenizer(self):
        return RegexpTokenizer(r"\w+")
